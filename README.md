
Real-time **chord/key/tempo listener** + an offline pipeline to **fine-tune a small transformer on MIDI** solos so it can generate solos conditioned on chord progressions.

## Repo Layout

SOLOMUSE-MAIN/
‚îú‚îÄ‚îÄ chords/
‚îÇ   ‚îî‚îÄ‚îÄ live_listener.py
‚îÇ
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ local/        # put your own MIDIs here (untracked)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ remote/       # datasets you download (untracked)
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prepare_dataset.py
‚îÇ   ‚îú‚îÄ‚îÄ finetuning/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ finetune_midi_gpt.py
‚îÇ   ‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ example.env
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md

> If you still have older names like `Chord Detection/` or `data prep/`, rename to the above for consistency.

## Quickstart

### 1 Install Python deps

```bash
pip install -r requirements.txt
```

On Apple Silicon (M1/M2/M3), PyTorch will use MPS (Metal) by default when available.

2) Install the Vamp Plugin Pack (for Chordino)
	‚Ä¢	macOS installer: Vamp Plugin Pack (includes nnls-chroma / Chordino).
	‚Ä¢	After install, set VAMP_PATH to the folder containing the plugins:
	‚Ä¢	Typically one of:
	‚Ä¢	~/Library/Audio/Plug-Ins/Vamp
	‚Ä¢	/Library/Audio/Plug-Ins/Vamp

You can store this in a .env (see below).

3) .env config

Copy the template and edit:

cp training/configs/example.env .env

Open .env and set:
	‚Ä¢	VAMP_PATH ‚Üí path to your Vamp plugins
	‚Ä¢	AUDIO_INPUT_DEVICE_NAME ‚Üí your mic or loopback device (e.g., ‚ÄúMacBook Pro Microphone‚Äù or ‚ÄúBlackHole 2ch‚Äù)
	‚Ä¢	Optional: WINDOW_SECONDS, SERVER_URL

4) Run the live listener

python chords/live_listener.py

You should see a stream like:

üé∂ Now playing: Em7 | Key: E minor | Tempo: 110.0 BPM

Tip: If you want to analyze system audio (e.g., a YouTube backing track), install a loopback device (e.g., BlackHole 2ch) and set AUDIO_INPUT_DEVICE_NAME=BlackHole 2ch in .env. To listen yourself while routing, create a macOS Audio MIDI Setup Multi-Output Device that includes both BlackHole and your headphones.

‚∏ª

Training (overview)
	‚Ä¢	Put MIDI files into training/data/local/ (your own) and/or download a public dataset into training/data/remote/.
	‚Ä¢	Run training/preprocessing/prepare_dataset.py to extract chords and solo phrases and convert them to event tokens with miditok.
	‚Ä¢	Run training/finetuning/finetune_midi_gpt.py to LoRA fine-tune a compact GPT-style model on those tokens.
	‚Ä¢	Models and logs are written to training/runs/‚Ä¶.

See the full guide in training/README.mdÔøº.

License & Dataset Notes
	‚Ä¢	This repo is MIT for code (unless you change it).
	‚Ä¢	Verify license/usage for any datasets you download (e.g., Lakh MIDI Dataset, MIDIWorld). Use only content you‚Äôre permitted to process.

---

### `training/README.md`

```markdown
# Training: Building a Solo Generator from MIDI

This subproject prepares a dataset of **(chords ‚Üí solo tokens)** pairs and fine-tunes a compact transformer with LoRA to generate solos conditioned on chord progressions.

## 0) Environment

Install repo-wide deps:

```bash
pip install -r ../requirements.txt
```

Create an env file:

cp configs/example.env ../../.env

Then update values in ../../.env as needed (paths, device name, etc.).

1) Data

Put MIDI files here:

training/data/
‚îú‚îÄ‚îÄ local/     # your MIDIs
‚îî‚îÄ‚îÄ remote/    # downloaded datasets (e.g., LMD subsets)

Suggested public datasets
	‚Ä¢	Lakh MIDI Dataset (LMD-matched or LMD-aligned): good quality; aligned subset is cleaner for timing.
	‚Ä¢	Start small (a few thousand files) to iterate faster.

‚ö†Ô∏è Check the dataset‚Äôs license/terms before use.

2) Prepare the dataset

This script:
	‚Ä¢	Parses MIDI,
	‚Ä¢	Splits into accompaniment vs. lead (heuristics),
	‚Ä¢	Extracts chord progressions and solo phrases,
	‚Ä¢	Tokenizes with miditok (REMI-like) ‚Üí JSONL of sequence pairs.

Run:

python preprocessing/prepare_dataset.py \
  --in_dir data/local \
  --in_dir data/remote \
  --out_json data/processed/midi_events.jsonl \
  --min_tracks 2 \
  --max_len 2048

Key flags:
	‚Ä¢	--min_tracks 2: skip 1-track MIDIs (often monophonic or percussion-only).
	‚Ä¢	--max_len: truncation length for token sequences.

Output: data/processed/midi_events.jsonl with records like:

{
  "id": "song_000123",
  "chords_tokens": [ ... ],
  "solo_tokens": [ ... ],
  "meta": { "tempo": 110, "key": "E", "scale": "minor" }
}

3) Fine-tune the model (LoRA)

We fine-tune a compact GPT-style model on token sequences (no audio). Default: small decoder LM from ü§ó Transformers.

python finetuning/finetune_midi_gpt.py \
  --dataset_path data/processed/midi_events.jsonl \
  --output_dir runs/midi-gpt-lora \
  --base_model tiny-gpt \
  --epochs 3 \
  --batch_size 8 \
  --lr 2e-4 \
  --lora_r 16 \
  --lora_alpha 32 \
  --lora_dropout 0.05

Notes
	‚Ä¢	--base_model can be a small GPT-like architecture you define in the script (fastest to iterate). You can later swap to a HF model id if you prefer.
	‚Ä¢	On Apple Silicon, PyTorch uses MPS automatically when available.

Artifacts saved to training/runs/midi-gpt-lora/:
	‚Ä¢	adapter_config.json & adapter_model.safetensors (LoRA weights)
	‚Ä¢	tokenizer config & logs.

4) Generate / sanity check

After training, the script prints a few sample generations. You can also add a small generate.py to:
	‚Ä¢	Load LoRA adapters on the base model,
	‚Ä¢	Feed chord tokens,
	‚Ä¢	Decode a solo token sequence,
	‚Ä¢	Convert tokens ‚Üí MIDI,
	‚Ä¢	Save generated_solo.mid for listening in a DAW.

Tips
	‚Ä¢	Start with small subsets (1‚Äì5k MIDI files). Clean your data: remove all-drums, very short, or broken files.
	‚Ä¢	Keep sequence lengths modest (max_len 1024‚Äì2048) to avoid OOM and speed up iterations.
	‚Ä¢	Use LoRA for rapid experiments; switch to full fine-tune only if you really need it.

Troubleshooting
	‚Ä¢	torch install: If you hit issues, try pip install --upgrade pip first. On macOS ARM, the default PyPI wheels generally work; MPS is used automatically when available.
	‚Ä¢	miditok tokenization errors: log problematic file paths; skip and continue.

---

### `chords/README.md`

```markdown
# Live Chord/Key/Tempo Listener

Continuously records short windows from your selected input (mic or loopback) and prints:
- **Chord** (Chordino via Vamp)
- **Key** (quick chroma template)
- **Tempo** (librosa beat tracker)
- A few ‚Äúmood‚Äù descriptors (RMS, centroid, ZCR)

## Setup

1) Install deps:
```bash
pip install -r ../requirements.txt
```

	2.	Install Vamp Plugin Pack and set your VAMP_PATH in an .env at repo root (see training/configs/example.env).
	3.	Choose your input device:

	‚Ä¢	AUDIO_INPUT_DEVICE_NAME="MacBook Pro Microphone" ‚Üí live mic
	‚Ä¢	or install BlackHole 2ch and use AUDIO_INPUT_DEVICE_NAME="BlackHole 2ch" to capture system audio (YouTube/Spotify/etc.). Use a macOS Multi-Output Device if you also want to hear it.

Run

python live_listener.py

You‚Äôll see a live line updating per window like:

üé∂ Now playing: Em7 | Key: E minor | Tempo: 110.0 BPM

If you see ‚ÄúNo plugin found: nnls-chroma:chordino‚Äù, your VAMP_PATH isn‚Äôt pointing at the right folder. Typical macOS paths:
	‚Ä¢	~/Library/Audio/Plug-Ins/Vamp
	‚Ä¢	/Library/Audio/Plug-Ins/Vamp

