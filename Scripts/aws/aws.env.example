# ===== AWS basics =====
AWS_PROFILE=default                    # Your AWS CLI profile (aws configure list)
AWS_REGION=us-east-1                   # e.g., us-east-1, us-west-2

# Either set AMI_ID directly OR let the script find the latest by name filter.
# AMI_ID=ami-xxxxxxxxxxxxxxxxx
AMI_NAME_FILTER="Deep Learning AMI GPU PyTorch * (Ubuntu 22.04) *"

INSTANCE_TYPE=g5.xlarge                # GPU instance (g4dn.xlarge is cheaper)
VOLUME_SIZE_GB=100                     # EBS volume for code + data
SECURITY_GROUP_NAME=solomuse-train-sg  # Will be created if missing
KEY_NAME=solomuse-train-key            # Will be created if missing
SSH_USERNAME=ubuntu                    # DLAMI default

# ===== Repo to clone on the instance =====
REPO_URL=https://github.com/<your-username>/SoloMuse-main.git
GIT_BRANCH=main

# ===== Paths inside the repo on the instance =====
REMOTE_TRAIN_SCRIPT=Training/03.finetuning/train_token_model.py
REMOTE_DATASET=Training/datasets/slakh_micro/pairs.npz
REMOTE_SPLIT=Training/datasets/slakh_micro/split.json
REMOTE_VOCAB=Training/datasets/slakh_micro/vocab.json
TRAIN_OUT_DIR=Training/03.finetuning/outputs/minitok

# ===== Training hyperparams =====
TRAIN_EPOCHS=20
TRAIN_BATCH=16
TRAIN_LR=3e-4
TRAIN_D_MODEL=128
TRAIN_NHEAD=4
TRAIN_NLAYERS=4
DEVICE=cuda                              # use GPU on the instance

# ===== (Optional) S3 sync (if your script supports it) =====
S3_BUCKET=s3://your-bucket-name
S3_DATA_PREFIX=s3://your-bucket-name/datasets/slakh_micro
S3_OUTPUT_PREFIX=s3://your-bucket-name/outputs/minitok

# ===== (Optional) Cost controls =====
USE_SPOT=false                           # true/false if your script supports spot
MAX_PRICE_PER_HOUR=1.00                  # only used if spot is enabled